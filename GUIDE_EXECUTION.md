# Guide d'Ex√©cution - Benchmark REST

## üöÄ D√©marrage Rapide

### 1. Pr√©requis

V√©rifier que vous avez install√© :
- Java 17+
- Maven 3.6+
- Docker & Docker Compose
- JMeter 5.6+ (dans le PATH)

### 2. Initialisation

```bash
# Rendre les scripts ex√©cutables (Linux/Mac)
chmod +x setup.sh run-benchmark.sh

# Ex√©cuter le setup
./setup.sh
```

Le script `setup.sh` va :
- V√©rifier les pr√©requis
- Compiler les applications
- D√©marrer l'infrastructure (PostgreSQL, Prometheus, Grafana, InfluxDB)
- Initialiser la base de donn√©es

### 3. Ex√©cution des Benchmarks

#### Ex√©cuter tous les sc√©narios sur une variante

```bash
# Variant A (Jersey)
./run-benchmark.sh a all

# Variant C (Spring MVC)
./run-benchmark.sh c all

# Variant D (Spring Data REST)
./run-benchmark.sh d all
```

#### Ex√©cuter un sc√©nario sp√©cifique

```bash
# Sc√©nario READ-heavy sur Variant A
./run-benchmark.sh a 1-read-heavy

# Sc√©nario JOIN-filter sur Variant C
./run-benchmark.sh c 2-join-filter

# Sc√©nario MIXED sur Variant D
./run-benchmark.sh d 3-mixed-writes

# Sc√©nario HEAVY-body sur Variant A
./run-benchmark.sh a 4-heavy-body
```

### 4. Consultation des R√©sultats

#### Rapports JMeter HTML

Les rapports sont g√©n√©r√©s automatiquement dans :
```
jmeter/results/[scenario]-[variant]-report/index.html
```

Ouvrir dans un navigateur pour voir :
- Summary Report (RPS, latence, erreurs)
- Response Times Over Time
- Throughput Over Time
- Custom Graphs

#### M√©triques Prometheus

**Grafana** : http://localhost:3000
- Login : `admin` / `admin`
- Dashboards pr√©-configur√©s pour JVM

**Prometheus** : http://localhost:9090
- Requ√™tes PromQL pour m√©triques d√©taill√©es

#### M√©triques Export√©es

Les m√©triques sont export√©es dans :
```
jmeter/results/metrics-[variant]-[timestamp].txt
```

---

## üìä Collecte des Donn√©es pour les Tableaux

### T2 - R√©sultats JMeter

1. Ouvrir le rapport HTML : `jmeter/results/[scenario]-[variant]-report/index.html`
2. Section **"Summary Report"** :
   - **RPS** = Throughput (req/s)
   - **p50** = Median (ms)
   - **p95** = 90th pct (ms)
   - **p99** = 99th pct (ms)
   - **Err %** = Error % (KO Rate)

### T3 - Ressources JVM

#### Via Grafana

1. Se connecter √† Grafana : http://localhost:3000
2. Ouvrir le dashboard JVM
3. Noter les valeurs moyennes et pics pour :
   - CPU (%)
   - Heap (MB)
   - GC Time (ms/s)
   - Threads actifs
   - HikariCP connections

#### Via Prometheus (PromQL)

**CPU** :
```promql
rate(process_cpu_seconds_total{job="variant-a-jersey"}[5m]) * 100
```

**Heap** :
```promql
jvm_memory_used_bytes{job="variant-a-jersey", area="heap"} / 1024 / 1024
```

**GC Time** :
```promql
rate(jvm_gc_pause_seconds_sum{job="variant-a-jersey"}[5m]) * 1000
```

**Threads** :
```promql
jvm_threads_live_threads{job="variant-a-jersey"}
```

**HikariCP** :
```promql
hikaricp_connections_active{job="variant-c-spring-mvc"}
hikaricp_connections_max{job="variant-c-spring-mvc"}
```

### T4 & T5 - D√©tails par Endpoint

Dans le rapport JMeter HTML :
1. Section **"Custom Graphs"** ou **"Response Times Over Time"**
2. Filtrer par nom de requ√™te HTTP
3. Extraire les m√©triques pour chaque endpoint

**Pour les observations (JOIN, N+1)** :
- Activer temporairement `show-sql: true` dans `application.yml`
- Observer les logs SQL g√©n√©r√©s
- Compter le nombre de requ√™tes pour d√©tecter N+1

### T6 - Incidents / Erreurs

Dans le rapport JMeter HTML :
1. Section **"Errors"**
2. Noter le type d'erreur (HTTP 500, timeout, etc.)
3. Calculer le pourcentage d'erreurs

---

## üîß Configuration du Mode Optimized vs Baseline

### Variant A (Jersey)

```bash
# Mode optimized (JOIN FETCH)
docker-compose --profile variant-a up -e QUERY_MODE=optimized

# Mode baseline (sans JOIN FETCH)
docker-compose --profile variant-a up -e QUERY_MODE=baseline
```

### Variants C & D (Spring)

Modifier `application.yml` :
```yaml
app:
  query:
    mode: optimized  # ou baseline
```

Ou via variable d'environnement :
```bash
QUERY_MODE=baseline docker-compose --profile variant-c up
```

---

## üìù Workflow Recommand√©

### 1. Pr√©paration

```bash
# Setup initial
./setup.sh

# V√©rifier que tout fonctionne
curl http://localhost:8081/categories?page=0&size=1
curl http://localhost:8082/categories?page=0&size=1
curl http://localhost:8083/categories?page=0&size=1
```

### 2. Ex√©cution des Tests

**Pour chaque variante (A, C, D)** :

```bash
# 1. Mode optimized
export QUERY_MODE=optimized
./run-benchmark.sh a all

# 2. Mode baseline (optionnel, pour comparaison)
export QUERY_MODE=baseline
./run-benchmark.sh a all
```

**Ordre recommand√©** :
1. Variant A - tous les sc√©narios
2. Variant C - tous les sc√©narios
3. Variant D - tous les sc√©narios

### 3. Collecte des Donn√©es

Pour chaque run :
1. ‚úÖ Noter les r√©sultats JMeter (T2)
2. ‚úÖ Exporter les m√©triques Prometheus (T3)
3. ‚úÖ Analyser les endpoints individuels (T4, T5)
4. ‚úÖ Noter les incidents (T6)

### 4. Analyse et Synth√®se

1. Comparer les r√©sultats entre variantes
2. Identifier les patterns (ex. Variant A meilleur pour X, Variant D meilleur pour Y)
3. Remplir T7 avec vos conclusions

---

## ‚ö†Ô∏è Points d'Attention

### Isolation des Tests

**Important** : Ne tester qu'une seule variante √† la fois !

```bash
# Arr√™ter les autres variantes avant de tester
docker-compose --profile variant-a down
docker-compose --profile variant-c down
docker-compose --profile variant-d down

# Puis d√©marrer celle √† tester
docker-compose --profile variant-a up -d
```

### Warm-up

Laisser l'application tourner 1-2 minutes avant de commencer les tests JMeter pour :
- Charger les classes
- Initialiser les pools de connexions
- Optimiser le JIT compiler

### R√©p√©tabilit√©

Pour des r√©sultats fiables :
- Ex√©cuter chaque sc√©nario 2-3 fois
- Prendre la moyenne des r√©sultats
- Noter les conditions (CPU/RAM disponible, autres processus)

### Nettoyage

Apr√®s chaque run :
```bash
# Arr√™ter la variante test√©e
docker-compose --profile variant-a down

# Nettoyer les r√©sultats si n√©cessaire
# (garder les rapports HTML importants)
```

---

## üêõ D√©pannage

### L'application ne d√©marre pas

```bash
# V√©rifier les logs
docker logs benchmark-variant-a

# V√©rifier que PostgreSQL est pr√™t
docker exec benchmark-postgres pg_isready -U benchmark
```

### JMeter ne trouve pas les fichiers CSV

V√©rifier les chemins dans les fichiers .jmx :
- Les chemins sont relatifs au r√©pertoire `jmeter/scenarios/`
- Format : `../test-data/categories.csv`

### Prometheus ne collecte pas les m√©triques

```bash
# V√©rifier que Prometheus peut acc√©der aux services
curl http://localhost:9091/metrics  # Variant A
curl http://localhost:8082/actuator/prometheus  # Variant C
curl http://localhost:8083/actuator/prometheus  # Variant D

# V√©rifier la configuration Prometheus
cat docker/prometheus.yml
```

### Erreurs de connexion √† la base de donn√©es

```bash
# V√©rifier que PostgreSQL est d√©marr√©
docker ps | grep postgres

# V√©rifier les variables d'environnement
docker exec benchmark-variant-a env | grep DB_
```

---

## üìö Ressources

- **Rapports JMeter** : `jmeter/results/`
- **M√©triques Prometheus** : http://localhost:9090
- **Dashboards Grafana** : http://localhost:3000
- **Logs Docker** : `docker logs [container-name]`

---

*Guide cr√©√© pour faciliter l'ex√©cution des benchmarks et la collecte des donn√©es*

